{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Handwritten Digits with VAEs in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, random_split, DataLoader\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import math\n",
    "from PIL import Image\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = datasets.MNIST(root='./mnist_data/', train=True, transform=transforms.ToTensor(), download=True)\n",
    "y_data = datasets.MNIST(root='./mnist_data/', train=False, transform=transforms.ToTensor(), download=False)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=x_data, batch_size=100, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=y_data, batch_size=100, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, latent_size=4):\n",
    "        super(VAE, self).__init__()\n",
    "        \n",
    "        self.latent_size = latent_size\n",
    "        \n",
    "        self.l1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=4, stride=2, padding=1)\n",
    "        self.l2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2, padding=1)        \n",
    "        \n",
    "        self.l21 = nn.Linear(32*2*7*7, self.latent_size)\n",
    "        self.l22 = nn.Linear(32*2*7*7, self.latent_size)\n",
    "        \n",
    "        self.f = nn.Linear(self.latent_size, 32*2*7*7)\n",
    "        \n",
    "        self.l3 = nn.ConvTranspose2d(in_channels=64, out_channels=32, kernel_size=4, stride=2, padding=1)\n",
    "        self.l4 = nn.ConvTranspose2d(in_channels=32, out_channels=1, kernel_size=4, stride=2, padding=1)\n",
    "        \n",
    "    def encoder(self, x_in):\n",
    "        h = F.relu(self.l1(x_in))\n",
    "        h = F.relu(self.l2(h))\n",
    "        \n",
    "        h = h.view(h.size(0), -1)\n",
    "        \n",
    "        return self.l21(h), self.l22(h)\n",
    "    \n",
    "    def decoder(self, z):\n",
    "        z = self.f(z)\n",
    "        z = z.view(z.size(0), 32*2, 7, 7)\n",
    "        \n",
    "        z = F.relu(self.l3(z))\n",
    "        z = torch.sigmoid(self.l4(z))\n",
    "        \n",
    "        return z\n",
    "    \n",
    "    def sampling(self, mu, log_var):\n",
    "        std = torch.exp(0.5*log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        return torch.add(eps.mul(std), mu)\n",
    "    \n",
    "    def forward(self, x_in):\n",
    "        mu, log_var = self.encoder(x_in)\n",
    "        z = self.sampling(mu, log_var)\n",
    "        return self.decoder(z), mu, log_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VAE(\n",
       "  (l1): Conv2d(1, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (l2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (l21): Linear(in_features=3136, out_features=4, bias=True)\n",
       "  (l22): Linear(in_features=3136, out_features=4, bias=True)\n",
       "  (f): Linear(in_features=4, out_features=3136, bias=True)\n",
       "  (l3): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (l4): ConvTranspose2d(32, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae = VAE()\n",
    "    \n",
    "vae.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 32, 14, 14]             544\n",
      "            Conv2d-2             [-1, 64, 7, 7]          32,832\n",
      "            Linear-3                    [-1, 4]          12,548\n",
      "            Linear-4                    [-1, 4]          12,548\n",
      "            Linear-5                 [-1, 3136]          15,680\n",
      "   ConvTranspose2d-6           [-1, 32, 14, 14]          32,800\n",
      "   ConvTranspose2d-7            [-1, 1, 28, 28]             513\n",
      "================================================================\n",
      "Total params: 107,465\n",
      "Trainable params: 107,465\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.15\n",
      "Params size (MB): 0.41\n",
      "Estimated Total Size (MB): 0.56\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(vae, (1, 28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(vae.parameters())\n",
    "\n",
    "def loss_function(recon_x, x, mu, log_var):\n",
    "    #print(recon_x.size())\n",
    "    #print(x.size())\n",
    "    BCE = F.binary_cross_entropy(recon_x, x, reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp()) # KL Divergence from MIT 6.S191\n",
    "    return (BCE + KLD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    vae.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        r_batch, mu, log_var = vae(data)\n",
    "\n",
    "        loss = loss_function(r_batch, data, mu, log_var)\n",
    "\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        \n",
    "    print('Epoch: {} Train mean loss: {:.8f}'.format(epoch, train_loss / len(train_loader.dataset)), end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(epoch):\n",
    "    vae.eval()\n",
    "    test_loss=0\n",
    "    with torch.no_grad():\n",
    "        for data, _ in test_loader:\n",
    "            data = data.to(device)\n",
    "            #print(data.size())\n",
    "            r, mu, log_var = vae(data)\n",
    "            #print(r.size())\n",
    "            #print(data.size())\n",
    "            test_loss += loss_function(r, data, mu, log_var).item()\n",
    "            \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('Epoch: {} Test mean loss: {:.8f}'.format(epoch, test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Train mean loss: 172.63472850 Epoch: 1 Test mean loss: 148.49528750\n",
      "Epoch: 2 Train mean loss: 144.33558420 Epoch: 2 Test mean loss: 141.12898662\n",
      "Epoch: 3 Train mean loss: 139.76057521 Epoch: 3 Test mean loss: 138.40836152\n",
      "Epoch: 4 Train mean loss: 137.40822023 Epoch: 4 Test mean loss: 136.47317852\n",
      "Epoch: 5 Train mean loss: 136.01164126 Epoch: 5 Test mean loss: 135.31066396\n",
      "Epoch: 6 Train mean loss: 134.97169360 Epoch: 6 Test mean loss: 134.51169756\n",
      "Epoch: 7 Train mean loss: 134.14543091 Epoch: 7 Test mean loss: 134.00177539\n",
      "Epoch: 8 Train mean loss: 133.52039969 Epoch: 8 Test mean loss: 133.63631113\n",
      "Epoch: 9 Train mean loss: 132.97341929 Epoch: 9 Test mean loss: 133.19777441\n",
      "Epoch: 10 Train mean loss: 132.54359199 Epoch: 10 Test mean loss: 132.77700938\n"
     ]
    }
   ],
   "source": [
    "n_epoches = 10\n",
    "\n",
    "for epoch in range(1, n_epoches+1):\n",
    "    train(epoch)\n",
    "    test(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    z = torch.randn(1, 4).to(device)\n",
    "    for i in range(100):\n",
    "        z = torch.add(z, 0.05)\n",
    "        \n",
    "        sample = vae.decoder(z).to(device)\n",
    "        save_image(sample.view(1, 28, 28), './samplesCVAE/sample' + str(i) + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
